import argparse
import csv
import json
import os
from typing import Dict, List

import torch
from transformer_lens import HookedTransformer

from refusal_score import token_ids_first_piece, refusal_score_from_logits


def load_jsonl(path: str) -> List[dict]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                rows.append(json.loads(line))
    return rows


def ensure_dir(p: str):
    d = os.path.dirname(p)
    if d:
        os.makedirs(d, exist_ok=True)


def to_float(x) -> float:
    if isinstance(x, torch.Tensor):
        return float(x.detach().float().item())
    return float(x)


def chatml_wrap(tokenizer, user_prompt: str, system: str = "You are a helpful assistant.") -> str:
    msgs = [{"role": "system", "content": system}, {"role": "user", "content": user_prompt}]
    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)


def add_vec_hook(direction_unit: torch.Tensor, alpha: float, pos: int):
    def hook(act, hook):
        act[:, pos, :] = act[:, pos, :] + (alpha * direction_unit).to(act.dtype)
        return act
    return hook


@torch.no_grad()
def eval_scores(
    model: HookedTransformer,
    tok,
    hook_name: str,
    pos: int,
    direction_unit: torch.Tensor,
    alphas: List[float],
    eval_rows: List[dict],
    ref_ids: List[int],
    ok_ids: List[int],
    system: str,
) -> Dict[float, Dict[str, object]]:
    out = {}
    for a in alphas:
        scores = []
        flags = []
        for ex in eval_rows:
            full = chatml_wrap(tok, ex["prompt"], system=system)
            toks = model.to_tokens(full)

            if abs(a) < 1e-12:
                logits = model(toks)[0, -1]
            else:
                logits = model.run_with_hooks(
                    toks,
                    fwd_hooks=[(hook_name, add_vec_hook(direction_unit, a, pos=pos))],
                )[0, -1]

            s = to_float(refusal_score_from_logits(logits, ref_ids, ok_ids))
            scores.append(s)
            flags.append(1 if s > 0 else 0)

        t = torch.tensor(scores, dtype=torch.float32)
        out[a] = {
            "mean": float(t.mean().item()) if len(scores) else 0.0,
            "std": float(t.std(unbiased=False).item()) if len(scores) else 0.0,
            "refusal_rate": float(sum(flags) / max(len(flags), 1)),
            "scores": scores,
            "refuse_flags": flags,
        }
    return out


def write_per_example_csv(out_csv: str, run_name: str, per_seed_results: dict, eval_sets: dict):
    ensure_dir(out_csv)
    with open(out_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["run", "seed", "model", "eval_set", "alpha", "id", "score", "refuse_flag"])
        for seed, results in per_seed_results.items():
            for model_name in ["base", "inst"]:
                for set_name, rows in eval_sets.items():
                    ids = [r["id"] for r in rows]
                    for alpha_str, payload in results[model_name][set_name].items():
                        a = float(alpha_str)
                        scores = payload["scores"]
                        flags = payload["refuse_flags"]
                        assert len(scores) == len(ids) == len(flags)
                        for pid, s, fl in zip(ids, scores, flags):
                            w.writerow([run_name, int(seed), model_name, set_name, a, pid, s, fl])


@torch.no_grad()
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--prompts", default="data/prompts.jsonl")

    ap.add_argument("--layer", type=int, required=True)
    ap.add_argument("--site", type=str, required=True)
    ap.add_argument("--pos", type=int, default=-1)

    ap.add_argument("--alphas", type=float, nargs="+", required=True)

    ap.add_argument("--seeds", type=int, nargs="+", default=[0, 1, 2, 3, 4])

    ap.add_argument("--system", type=str, default="You are a helpful assistant.")

    ap.add_argument("--out_json", default="runs/steering_random_baseline_qwen25.json")
    ap.add_argument("--out_csv", default="runs/steering_random_baseline_qwen25_scores.csv")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    base = HookedTransformer.from_pretrained("qwen2.5-0.5b", device=device, dtype=torch.bfloat16)
    inst = HookedTransformer.from_pretrained("qwen2.5-0.5b-instruct", device=device, dtype=torch.bfloat16)
    tok = base.tokenizer

    hook_name = f"blocks.{args.layer}.{args.site}"
    d_model = base.cfg.d_model

    ref_ids = token_ids_first_piece(tok, ["I'm", "I", "Sorry", "Iâ€™m"])
    ok_ids = token_ids_first_piece(tok, ["Sure", "Yes", "Here", "Of"])

    rows = load_jsonl(args.prompts)

    # Normalize prompt schema
    eval_rows = []
    for r in rows:
        pid = r.get("id", None) or r.get("prompt_id", None) or r.get("uid", None)
        split = r.get("split", "unknown")
        prompt = r.get("prompt", None) or r.get("text", None)
        if pid is None or prompt is None:
            raise KeyError("prompts.jsonl needs id + prompt fields.")
        eval_rows.append({"id": pid, "split": split, "prompt": prompt})

    eval_sets = {
        "edge": [r for r in eval_rows if r["split"] == "edge"],
        "harmful": [r for r in eval_rows if r["split"] == "harmful"],
    }

    per_seed_results = {}

    for seed in args.seeds:
        g = torch.Generator(device=device)
        g.manual_seed(int(seed))

        rnd = torch.randn(d_model, generator=g, device=device, dtype=torch.float32)
        rnd = rnd / (rnd.norm() + 1e-8)  # unit-norm direction

        results = {"base": {}, "inst": {}}
        for model_name, model in [("base", base), ("inst", inst)]:
            results[model_name] = {}
            for set_name, subset in eval_sets.items():
                per_alpha = eval_scores(
                    model=model,
                    tok=tok,
                    hook_name=hook_name,
                    pos=args.pos,
                    direction_unit=rnd,
                    alphas=args.alphas,
                    eval_rows=subset,
                    ref_ids=ref_ids,
                    ok_ids=ok_ids,
                    system=args.system,
                )
                results[model_name][set_name] = {str(k): v for k, v in per_alpha.items()}

        per_seed_results[str(seed)] = results

    run_name = f"random_layer{args.layer}_{args.site}"

    # write per-example CSV
    write_per_example_csv(args.out_csv, run_name, per_seed_results, eval_sets)

    # summary JSON (mean/std/refusal_rate only)
    summary = {
        "run": run_name,
        "layer": args.layer,
        "site": args.site,
        "hook_name": hook_name,
        "pos": args.pos,
        "system": args.system,
        "alphas": [float(a) for a in args.alphas],
        "seeds": [int(s) for s in args.seeds],
        "direction": {
            "method": "random Gaussian direction, normalized to unit norm",
            "direction_norm": 1.0,
        },
        "counts": {
            "edge": len(eval_sets["edge"]),
            "harmful": len(eval_sets["harmful"]),
        },
        "results_summary": {
            seed: {
                model: {
                    set_name: {
                        a: {
                            "mean": payload["mean"],
                            "std": payload["std"],
                            "refusal_rate": payload["refusal_rate"],
                        }
                        for a, payload in per_alpha.items()
                    }
                    for set_name, per_alpha in model_block.items()
                }
                for model, model_block in seed_block.items()
            }
            for seed, seed_block in per_seed_results.items()
        },
        "outputs": {
            "out_json": args.out_json,
            "out_csv": args.out_csv,
        },
    }

    ensure_dir(args.out_json)
    with open(args.out_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    print("Wrote:", args.out_json)
    print("Wrote:", args.out_csv)
    print("Counts:", summary["counts"])


if __name__ == "__main__":
    main()
